{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WakeMe_CreateSummaryTablesforDatasets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWpt_lNIcA7A"
      },
      "source": [
        "This jupyter notebooks process the databases and creates summary tables. This tables are being used to define database to be used during training.\n",
        "\n",
        "Expected Datasets:\n",
        "\n",
        "/dataset/cv-corpus-6.1-2020-12-11\n",
        "\n",
        "/dataset/speech_commands_v0.01_full\n",
        "\n",
        "/dataset/fourthbrain\n",
        "\n",
        "/dataset/heyfourthbrain\n",
        "\n",
        "/dataset/hellofourthbrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfySlnfOj1Lx"
      },
      "source": [
        "# Load relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPHDB2CqcNst",
        "outputId": "8996643d-4870-495a-caae-2b1acfd606f0"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import warnings\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "import glob\n",
        "import shutil\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
        "\n",
        "import datetime\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "import soundfile\n",
        "\n",
        "!pip install textgrid\n",
        "import textgrid\n",
        "\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textgrid\n",
            "  Downloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: textgrid\n",
            "Successfully installed textgrid-1.5\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1G6Wd5Hk51B",
        "outputId": "1ac97143-c21c-40f2-8cc7-014f061896c1"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QUOXWVhlLwO"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/SpotifyFourthBrainPartnerProject/custom_wakeword_engine/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrWtstg4aFXl"
      },
      "source": [
        "database_folder = 'datasets'\n",
        "create_summary = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT7fgdSl4Atw"
      },
      "source": [
        "# Create summary  for 'hey fourthbrain' database\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCLuwxEO4EMz"
      },
      "source": [
        "if (create_summary == True):\n",
        "  heyfourthbrain_files = glob.glob(database_folder+'/heyfourthbrain/*.wav')\n",
        "  heyfourthbrain_validated = database_folder+'/heyfourthbrain/heyfourthbrain_validated.txt'\n",
        "\n",
        "  df = pd.DataFrame(columns = ['path', 'filename','sentence', 'label', 'lentgh(sec)'])\n",
        "\n",
        "  for i in range(len(heyfourthbrain_files)):\n",
        "    sounddata = librosa.core.load(heyfourthbrain_files[i], sr=16000, mono=True)[0]\n",
        "    df = df.append({'path' : heyfourthbrain_files[i],'filename' : heyfourthbrain_files[i].split('/')[-1], 'sentence' : ' heyfourthbrain ','label' : 'heyfourthbrain', 'lentgh(sec)' : len(sounddata)/16000},ignore_index = True)\n",
        "\n",
        "  df.to_csv(heyfourthbrain_validated,sep='\\t',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmsfjbyHktEB"
      },
      "source": [
        "# Create summary  for 'fourthbrain' database\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqyuzNp5tnaL"
      },
      "source": [
        "if (create_summary == True):\n",
        "  fourthbrain_files = glob.glob(database_folder+'/fourthbrain/*.wav')\n",
        "  fourthbrain_validated = database_folder+'/fourthbrain/fourthbrain_validated.txt'\n",
        "\n",
        "  df = pd.DataFrame(columns = ['path', 'filename','sentence', 'label', 'lentgh(sec)'])\n",
        "\n",
        "  for i in range(len(fourthbrain_files)):\n",
        "    sounddata = librosa.core.load(fourthbrain_files[i], sr=16000, mono=True)[0]\n",
        "    df = df.append({'path' : fourthbrain_files[i],'filename' : fourthbrain_files[i].split('/')[-1], 'sentence' : ' fourthbrain ','label' : 'fourthbrain', 'lentgh(sec)' : len(sounddata)/16000},ignore_index = True)\n",
        "\n",
        "  df.to_csv(fourthbrain_validated,sep='\\t',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzdpiReA557T"
      },
      "source": [
        "# Create summary  for MCV data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LB-2BuhjR8y"
      },
      "source": [
        "if (create_summary == True):\n",
        "\n",
        "  mcv = database_folder+'/cv-corpus-6.1-2020-12-11/validated.tsv'\n",
        "  mcv_validated = database_folder+'/cv-corpus-6.1-2020-12-11/mcv_validated.tsv'\n",
        "  mcv_negtest = database_folder+'/cv-corpus-6.1-2020-12-11/mcv_negtest.tsv'\n",
        "\n",
        "  df = pd.read_csv(mcv,sep='\\t')\n",
        "\n",
        "  df ['filename'] = df['path']\n",
        "  df ['path'] = database_folder+'/cv-corpus-6.1-2020-12-11/subset_cv/'+ df ['filename'] \n",
        "  df ['label'] = 'NA'\n",
        "  df = df[['path', 'filename','sentence', 'label']]\n",
        "\n",
        "  # check which audio files are copied from full database\n",
        "  candid_paths = [filename.split('/')[-1] for filename in glob.glob('datasets/cv-corpus-6.1-2020-12-11/subset_cv/*.mp3') ]\n",
        "\n",
        "  df = df[df['filename'].isin(candid_paths)].reset_index(drop=True)\n",
        "  df_negtest = df.loc[0:1000]\n",
        "  df_val = df.loc[1001:]\n",
        "\n",
        "  df_val.to_csv(mcv_validated,sep='\\t',index=False)\n",
        "  df_negtest.to_csv(mcv_negtest,sep='\\t',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1f9a5p8a_Tk"
      },
      "source": [
        "# Create summary for SpeechCommand (SC) data set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by3htUeZbQsp"
      },
      "source": [
        "if (create_summary == True):\n",
        "\n",
        "# Find labels for all files and save them as a datasummary in .csv file\n",
        "  sc_labels = [ name for name in os.listdir(database_folder+'/speech_commands_v0.01_full') if os.path.isdir(os.path.join(database_folder+'/speech_commands_v0.01_full', name)) ]\n",
        "  df = pd.DataFrame(columns=['path', 'filename','sentence', 'label','lentgh(sec)'])\n",
        "  sc_validated = database_folder+'/speech_commands_v0.01_full/sc_validated.tsv'\n",
        "  for sc_label in sc_labels:\n",
        "      file_names = os.listdir(database_folder+'/speech_commands_v0.01_full/'+sc_label)\n",
        "      for file_name in file_names:\n",
        "        #if ((sc_label+'/'+file_name) in val_list):\n",
        "        path_name = database_folder+'/speech_commands_v0.01_full/'+sc_label+'/'+file_name\n",
        "        df = df.append({'path' : path_name, 'filename' : file_name,  'sentence' : ' '+sc_label+' ', 'label' : sc_label,'lentgh(sec)' : 1},ignore_index = True)\n",
        "  df.to_csv(sc_validated,sep='\\t',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl-3fBJOLaD1"
      },
      "source": [
        "# Create summary  for 'hello fourthbrain' database\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j8Zd_H4VZlR"
      },
      "source": [
        "if (create_summary == True):\n",
        "  hellofourthbrain_files = glob.glob(database_folder+'/hellofourthbrain/*.wav')\n",
        "  hellofourthbrain_validated = database_folder+'/hellofourthbrain/hellofourthbrain_validated.txt'\n",
        "\n",
        "  df = pd.DataFrame(columns = ['path', 'filename','sentence', 'label', 'lentgh(sec)'])\n",
        "\n",
        "  for i in range(len(hellofourthbrain_files)):\n",
        "    sounddata = librosa.core.load(hellofourthbrain_files[i], sr=16000, mono=True)[0]\n",
        "    df = df.append({'path' : hellofourthbrain_files[i],'filename' : hellofourthbrain_files[i].split('/')[-1], 'sentence' : ' hellofourthbrain ','label' : 'hellofourthbrain', 'lentgh(sec)' : len(sounddata)/16000},ignore_index = True)\n",
        "\n",
        "  df.to_csv(hellofourthbrain_validated,sep='\\t',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}